{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163dc993",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Exploration and Preparation\n",
    "\n",
    "**Objective:** This notebook provides an interactive guide to the initial data preparation workflow for the CT-RATE dataset. We will cover:\n",
    "\n",
    "1.  **Loading and Configuring**: Setting up the environment and loading the main configuration.\n",
    "2.  **Label Analysis**: Visualizing the distribution and co-occurrence of pathology labels.\n",
    "3.  **Data Filtering**: Replicating the logic for creating a filtered master list of volumes, excluding certain patients to prevent data leakage.\n",
    "4.  **K-Fold Split Generation**: Demonstrating how patient-aware, stratified cross-validation splits are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# --- Project-Specific Imports ---\n",
    "# To import from the 'src' directory, we need to add the project root to the Python path.\n",
    "# We assume this notebook is located in a 'notebooks' directory at the project root.\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src.config import load_config\n",
    "from scripts.data_preparation.create_filtered_dataset import get_patient_id, normalize_name_from_path, natural_sort_key\n",
    "from scripts.data_preparation.create_kfold_splits import create_kfold_splits\n",
    "\n",
    "# --- Notebook Setup ---\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\", font_scale=1.2)\n",
    "# This setting ensures that all columns are displayed in pandas DataFrames.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Imports successful and project path configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main configuration file to access all project paths and parameters.\n",
    "# This ensures that the notebook uses the same settings as the main application scripts.\n",
    "try:\n",
    "    config_path = project_root / 'configs' / 'config_example.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "    # Display a few key paths to verify\n",
    "    print(f\"Data Directory: {config.paths.data_dir}\")\n",
    "    print(f\"Labels File: {config.paths.labels.all}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find the configuration file.\")\n",
    "    print(\"Please ensure 'configs/config_example.yaml' exists and the project root is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12daf362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the primary data files: the full metadata and the complete set of labels.\n",
    "try:\n",
    "    all_labels_df = pd.read_csv(config.paths.labels.all)\n",
    "    \n",
    "    # We also need the metadata files to get the full list of available volumes.\n",
    "    train_metadata_df = pd.read_csv(config.paths.metadata.train)\n",
    "    valid_metadata_df = pd.read_csv(config.paths.metadata.valid)\n",
    "    all_volumes_df = pd.concat([train_metadata_df, valid_metadata_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    print(\"Full Label Set Info:\")\n",
    "    all_labels_df.info()\n",
    "    print(\"\\nFull Label Set Head:\")\n",
    "    display(all_labels_df.head())\n",
    "\n",
    "    # print(\"\\nAll Volumes (from metadata) Info:\")\n",
    "    # all_volumes_df.info()\n",
    "    print(\"\\nAll Volumes Head:\")\n",
    "    display(all_volumes_df.head())\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not load a required data file: {e}\")\n",
    "    print(\"Please ensure the paths in your config file are correct and the data files exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9185490",
   "metadata": {},
   "source": [
    "## Understanding the Dataset Hierarchy\n",
    "\n",
    "Before we analyze the labels, it is crucial to understand the structure of the CT-RATE dataset. The data is organized hierarchically:\n",
    "\n",
    "-   **Patients**: The highest level. A single patient may have multiple CT scans over time.\n",
    "-   **CT Scans**: An imaging session for a patient.\n",
    "-   **Volumes (or Reconstructions)**: A single CT scan can be reconstructed with different parameters (e.g., different slice thickness or kernels), resulting in multiple 3D volume files (`.nii.gz`) for the same scan.\n",
    "\n",
    "This means the total number of `.nii.gz` files is much larger than the number of unique scans, which in turn is larger than the number of unique patients. For our analysis, especially for splitting the data, we must operate at the **patient level** to prevent data leakage.\n",
    "\n",
    "Let's quantify this structure using our loaded metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e96ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate counts for each level of the hierarchy ---\n",
    "\n",
    "# 1. Total number of reconstructed volumes (each row in the metadata is one volume)\n",
    "total_volumes = len(all_volumes_df)\n",
    "\n",
    "# 2. Number of unique CT scans\n",
    "# We can identify a unique scan by its name minus the final reconstruction part (e.g., '_1', '_2')\n",
    "# 'train_123_a_1' -> 'train_123_a'\n",
    "all_volumes_df['ScanID'] = all_volumes_df['VolumeName'].str.rsplit('_', n=1).str[0]\n",
    "total_scans = all_volumes_df['ScanID'].nunique()\n",
    "\n",
    "# 3. Number of unique patients\n",
    "# The PatientID is the second part of the VolumeName, e.g., 'train_123_a_1' -> '123'\n",
    "all_volumes_df['PatientID'] = all_volumes_df['VolumeName'].apply(get_patient_id)\n",
    "total_patients = all_volumes_df['PatientID'].nunique()\n",
    "\n",
    "# --- Display the statistics ---\n",
    "print(\"--- CT-RATE Dataset Hierarchy Statistics ---\")\n",
    "print(f\"Total Reconstructed Volumes: {total_volumes:,}\")\n",
    "print(f\"Total Unique CT Scans:      {total_scans:,}\")\n",
    "print(f\"Total Unique Patients:        {total_patients:,}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Display the new columns to verify\n",
    "display(all_volumes_df[['VolumeName', 'ScanID', 'PatientID']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7266669",
   "metadata": {},
   "source": [
    "## 2. Label Distribution Analysis\n",
    "\n",
    "Before filtering, let's analyze the raw label data to understand its characteristics. We will visualize three key aspects:\n",
    "\n",
    "-   **Label Frequency**: How many times does each pathology appear in the dataset? This helps identify class imbalance.\n",
    "-   **Labels per Scan**: How many pathologies are typically assigned to a single CT scan?\n",
    "-   **Label Co-occurrence**: Which pathologies tend to appear together? This reveals potential clinical correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the refined plotting functions adapted from your EDA notebook.\n",
    "\n",
    "def plot_label_frequencies(df_labels, label_columns):\n",
    "    \"\"\"Generates a horizontal bar chart showing the frequency of each label.\"\"\"\n",
    "    label_counts = df_labels[label_columns].sum().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x=label_counts.values, y=label_counts.index, palette=\"viridis\", orient='h')\n",
    "    plt.xlabel(\"Frequency (Number of Positive Cases)\")\n",
    "    plt.ylabel(\"Pathology Label\")\n",
    "    plt.title(\"Frequency of Each Pathology Label\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_labels_per_scan_distribution(df_labels, label_columns):\n",
    "    \"\"\"Generates a histogram of the number of positive labels per scan.\"\"\"\n",
    "    labels_per_scan = df_labels[label_columns].sum(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(labels_per_scan, kde=False, color=\"skyblue\", discrete=True)\n",
    "    plt.xlabel(\"Number of Positive Labels per Scan\")\n",
    "    plt.ylabel(\"Number of Scans\")\n",
    "    plt.title(\"Distribution of Number of Labels per Scan\", fontsize=16)\n",
    "    plt.xticks(range(0, labels_per_scan.max() + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_label_cooccurrence_heatmap(df_labels, label_columns):\n",
    "    \"\"\"Generates a heatmap showing the co-occurrence of label pairs.\"\"\"\n",
    "    df_label_data = df_labels[label_columns]\n",
    "    cooccurrence_matrix = df_label_data.T.dot(df_label_data)\n",
    "    \n",
    "    # For visualization, we normalize by the diagonal to see conditional probabilities\n",
    "    # P(Y | X) = Count(X and Y) / Count(X)\n",
    "    diagonal_counts = np.diag(cooccurrence_matrix)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        normalized_matrix = cooccurrence_matrix.astype(float).div(diagonal_counts, axis=0)\n",
    "    normalized_matrix = normalized_matrix.fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(normalized_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\", linewidths=.5)\n",
    "    plt.xlabel(\"Given this Label is Present (X)\")\n",
    "    plt.ylabel(\"Probability of this Label also being Present (Y)\")\n",
    "    plt.title(\"Normalized Label Co-occurrence P(Y|X)\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get the list of pathology columns from the config file\n",
    "pathology_columns = config.pathologies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee5a3e",
   "metadata": {},
   "source": [
    "### Correcting for the Data Hierarchy\n",
    "\n",
    "As noted, the `all_labels_df` contains entries for every reconstructed volume, not every unique scan. To get an accurate view of the label distribution, we must first deduplicate the data so that each unique scan is represented only once. We will use the `ScanID` we created earlier for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda966a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Create the 'ScanID' in the labels DataFrame ---\n",
    "# As you correctly pointed out, we first need to create the ScanID in our labels DataFrame.\n",
    "# The logic is the same: strip the final reconstruction part from the VolumeName.\n",
    "all_labels_df['ScanID'] = all_labels_df['VolumeName'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# --- 2. Now, perform the deduplication ---\n",
    "# With the ScanID present, we can now drop duplicates to get one entry per unique scan.\n",
    "scan_level_labels_df = all_labels_df.drop_duplicates(subset=['ScanID']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(f\"Original number of label entries (volumes): {len(all_labels_df):,}\")\n",
    "print(f\"Deduplicated number of label entries (scans): {len(scan_level_labels_df):,}\")\n",
    "\n",
    "print(\"\\nHead of the new scan-level DataFrame with 'ScanID':\")\n",
    "display(scan_level_labels_df[['VolumeName', 'ScanID'] + pathology_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_frequencies(scan_level_labels_df, pathology_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_per_scan_distribution(scan_level_labels_df, pathology_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ebb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_cooccurrence_heatmap(scan_level_labels_df, pathology_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9f315",
   "metadata": {},
   "source": [
    "## 3. Executing the Data Filtering Script and creation of a Master List\n",
    "\n",
    "For the experiments in this repository, a crucial decision was made to separate the manually labeled scans from the main corpus. These manually labeled scans, which can be found at the [CT-CLIP repository](https://github.com/ibrahimethemhamamci/CT-CLIP/tree/main/text_classifier/data), are treated as a gold standard for evaluation.\n",
    "\n",
    "Furthermore, as noted in the official [CT-RATE dataset correction note](https://huggingface.co/datasets/ibrahimhamamci/CT-RATE/blob/main/dataset/data_correction_note.md), certain scans have been identified as brain scans or as having a missing z-space.\n",
    "\n",
    "For these reasons, all of these scans (manual labels, brain scans, missing z-space) are removed from the main body of data. The original train/validation split provided with the CT-RATE dataset on Hugging Face is ignored. Instead, this repository provides a `FILTERED_MASTER_LIST.csv`. This approach offers two main advantages:\n",
    "\n",
    "1.  **Flexibility**: Users of this repository can freely create their own data splits from a clean, reliable master list.\n",
    "2.  **Gold Standard Evaluation**: The separated manual labels can be used as a high-quality, independent test set.\n",
    "\n",
    "The trade-off is that results from this repository may not be directly comparable to other models trained on the original, unfiltered dataset splits. However, for the objectives of these experiments, having a reliable, manually verified evaluation set is of higher importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932950e",
   "metadata": {},
   "source": [
    "#### A Note on Implementation\n",
    "\n",
    "For the filtering script to function correctly, it is required that the user generates a unified CSV file containing the manual labels.\n",
    "\n",
    "This file, which corresponds to the `manual_labels` key in the `exclusion_files` section of the config, must contain a `VolumeName` column for each scan that has been manually labeled. It is highly recommended that this CSV also include the corresponding pathology labels and the full text report for each scan.\n",
    "\n",
    "The necessary mapping and labels to create this file can be found in the link provided above for the manual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Execute the filtering script function ---\n",
    "# This function encapsulates the entire filtering process.\n",
    "from scripts.data_preparation.create_filtered_dataset import create_filtered_dataset\n",
    "\n",
    "print(\"Running the create_filtered_dataset function...\")\n",
    "create_filtered_dataset(config)\n",
    "\n",
    "# --- 2. Load and inspect the generated file ---\n",
    "try:\n",
    "    filtered_list_path = Path(config.paths.data_dir) / config.paths.output_filename\n",
    "    filtered_df = pd.read_csv(filtered_list_path)\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded the filtered list from: {filtered_list_path}\")\n",
    "    \n",
    "    initial_volume_count = len(all_volumes_df)\n",
    "    final_volume_count = len(filtered_df)\n",
    "    \n",
    "    print(f\"\\nInitial number of volumes: {initial_volume_count}\")\n",
    "    print(f\"Number of volumes after filtering: {final_volume_count}\")\n",
    "    print(f\"Total volumes removed: {initial_volume_count - final_volume_count}\")\n",
    "\n",
    "    print(\"\\nHead of the final filtered list of volumes:\")\n",
    "    display(filtered_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The expected output file was not found at {filtered_list_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7d82f",
   "metadata": {},
   "source": [
    "## 4. Generating K-Fold Splits\n",
    "\n",
    "Now, using the filtered master list, we will call the `create_kfold_splits` function. This script will generate the stratified, patient-aware cross-validation folds and save them as separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66226503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define parameters and execute the splitting function ---\n",
    "from scripts.data_preparation.create_kfold_splits import create_kfold_splits\n",
    "\n",
    "N_SPLITS = 5\n",
    "output_dir = Path(config.paths.data_dir) / \"splits\" / f\"kfold_{N_SPLITS}\"\n",
    "\n",
    "print(f\"Running the create_kfold_splits function for {N_SPLITS} splits...\")\n",
    "# The config needs to be updated to point to the correct master list file.\n",
    "config.paths.full_dataset_csv = config.paths.output_filename\n",
    "create_kfold_splits(config, n_splits=N_SPLITS, output_dir=output_dir)\n",
    "\n",
    "# --- 2. Verify that the split files have been created ---\n",
    "print(f\"\\nChecking for output files in: {output_dir}\")\n",
    "assert output_dir.exists(), \"Output directory for splits was not created.\"\n",
    "\n",
    "# Check for the first fold's files\n",
    "train_path = output_dir / \"train_fold_0.csv\"\n",
    "valid_path = output_dir / \"valid_fold_0.csv\"\n",
    "\n",
    "assert train_path.exists(), \"train_fold_0.csv not found.\"\n",
    "assert valid_path.exists(), \"valid_fold_0.csv not found.\"\n",
    "print(\"Split files for Fold 0 found successfully.\")\n",
    "\n",
    "\n",
    "# --- 3. Load a sample fold and verify no patient leakage ---\n",
    "df_train_fold = pd.read_csv(train_path)\n",
    "df_valid_fold = pd.read_csv(valid_path)\n",
    "\n",
    "# Extract patient IDs\n",
    "df_train_fold['PatientID'] = df_train_fold['VolumeName'].apply(get_patient_id)\n",
    "df_valid_fold['PatientID'] = df_valid_fold['VolumeName'].apply(get_patient_id)\n",
    "\n",
    "train_patients = set(df_train_fold['PatientID'])\n",
    "valid_patients = set(df_valid_fold['PatientID'])\n",
    "\n",
    "print(f\"\\nNumber of patients in training set (Fold 0): {len(train_patients)}\")\n",
    "print(f\"Number of patients in validation set (Fold 0): {len(valid_patients)}\")\n",
    "\n",
    "assert train_patients.isdisjoint(valid_patients), \"Patient leakage detected!\"\n",
    "print(\"\\nVerification successful: No patient leakage between training and validation sets for Fold 0.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Memoria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
